# 消息队列：Kafka

当有一个用户注册完成之后，需要熊发送一封邮件，进行注册认证，在网络的繁忙的时候，进行网络邮件的发送与认证无疑会增加服务器和网络的负担。在[牛客网](https://www.nowcoder.com/)的中级项目使用Redis作为处理异步事件的消息队列，但是Redis是基于内存的，对于数据的及时写入需要配置设置文件，但这无疑会影响Redis作为数据库的使用效率，而且对于分布式的消息队列也不是Redis擅长的。笔者使用Kafka作为消息队列来处理异步事件，可以将注册用户的相关封装为消息，交给Kafka，后台使用专门的服务器来处理邮件发送和其他事务的处理。


核心代码如下：

```java
public class KafkaUtil {
    public static final String KAFKA_TOPIC_MAIL = "celtics_register";
    public static final String KAFKA_TOPIC_OTHER = "celtics_another";

    private static KafkaProducer<String, String> kafkaProducer;
    private static KafkaConsumer<String, String> kafkaConsumer;

    static {
        /*
            kafka生产者可以定义的属性：
                1. kafka服务的地址, 不需要将所有的broker指定上
                2. 消息被接收的确认信号
                        - 0表示不确认
                        - -1表示所有的follower都返回ack才确认
                        - 1表示接受到leader的ack返回确认
                3. 消息发送失败的重复次数
                4. 当多个producer向同一个partition发送消息, 要求消息以16384的大小发送, 从而减少交互次数
                5. 消息储存于发送缓冲区的时间
                6. 发送缓冲区最大值, server.properties默认值=102400
                7. partition对应的key的序列化类
                8. partition储存数据value的序列化类
         */

        Properties properties = new Properties();
        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "127.0.0.1:9091");
        properties.put(ProducerConfig.ACKS_CONFIG, "-1");
        properties.put(ProducerConfig.RETRIES_CONFIG, 1);
        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);
        properties.put(ProducerConfig.LINGER_MS_CONFIG, 10);
        properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 102400);
        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");

        kafkaProducer = new KafkaProducer<String, String>(properties);
    }

    static {
        /*
            kafka消费者可以定义的属性（自动确认offset方案）：
                1. kafka服务的地址, 不需要将所有的broker指定上
                2. 消费者组
                3. 是否自动确认offset
                4. 自动确认offset的时间间隔
                5. 会话超时时间（是不是指的是和zooKeeper的会话?） [6, 30]s
                6. partition对应的key的序列化类
                7. partition储存数据value的序列化类
         */

        Properties properties = new Properties();
        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "127.0.0.1:9092");
        properties.put(ConsumerConfig.GROUP_ID_CONFIG, "112233");
        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);
        properties.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, 1000);
        properties.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 30000);
        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");

        kafkaConsumer = new KafkaConsumer<String, String>(properties);
    }


    static {
        /* 订阅关注的topic */
        HashSet<String> topics = new HashSet<String>();
        topics.add(KafkaUtil.KAFKA_TOPIC_MAIL);
        topics.add(KafkaUtil.KAFKA_TOPIC_OTHER);
        kafkaConsumer.subscribe(topics);
    }

    public void send(String topic, String msg) {
        try{
            ProducerRecord<String, String> record = new ProducerRecord<String, String>(topic, msg);
            kafkaProducer.send(record);

        }catch (Exception e){
            e.printStackTrace();
        }
    }

    public void subscribe(Set<String> topics) {
        try{
            kafkaConsumer.subscribe(topics);
        }catch (Exception e){
            e.printStackTrace();
        }
    }

    public void consume(long timeout) {
        try{
            MailUtil mailUtil = new MailUtil();
            while (true) {
                ConsumerRecords<String, String> records = kafkaConsumer.poll(timeout);

                if (records != null) {
	                for (ConsumerRecord<String, String> record : records) {
	                    String topic = record.topic();
	                    String msg = record.value();
	                    if(topic.equals(KafkaUtil.KAFKA_TOPIC_MAIL)){
	                        UserModel user = JSON.parseObject(msg, UserModel.class);
	                        mailUtil.sendMail(user);
	                    }
	                    else if(topic.equals(KAFKA_TOPIC_OTHER)){
	                        //TODO ..处理其他的主题....
	                    }
	                    else{
	                        //TODO ..else if
	                    }
	                }
                }
            }
        }catch (Exception e){
            e.printStackTrace();
        }
    }
}
```